\newcommand{\red}[1]{{\color{red}{#1}}}


\section{Related Work and Background}
\paragraph{Action segmentation} The goal of action segmentation is to temporarily localize action segments and classify the category of the action in each segment in an untrimmed input video. The application of action segmentation can be found in various fields such as robotics \cite{robotics_action_seg} and behavior analysis \cite{shao2012human}. Action segmentation is closely related but different from action recognition and action detection. Action recognition identifies one action in a trimmed video, and action detection usually outputs a sparse set of actions. By contrast, action segmentation is a more complex task that considers a longer range of temporal relations between sequential activities for more fine-grained action recognition in an untrimmed video.

\paragraph{Early works}
Traditional approaches generally fall into three categories: sliding window approaches, segmental models, and recurrent networks \cite{graphbased2020}. One of the earliest attempts is to detect action segments with temporal windows of different scales and non-maximum suppression \cite{6247801}. However, this method is limited by the tradeoff between larger window size and computational costs. Others use segmental models like spatiotemporal CNNs with the semi-Markov model for tracking object relationships, action transitions, and environment change \cite{lea2016segmental, 6619177}. With each action conditioned on the previous one, these methods are good at capturing local dependencies in consecutive visual patterns rather than long-range temporal relations. Other hybrid approaches include representing frames using Fisher Vectors with HMMs and GRUs for temporal modeling \cite{7477701, 8099623}, which have their main drawback of efficiency. Another line of research focuses on temporal convolutional networks (TCNs) that perform fine-grained action segmentation using temporal convolutions \cite{8099596}. The method is extended to a multi-stage architecture with a set of dilated temporal
convolutions in each stage. It is proven to be able to avoid temporal pooling and better capture long-range dependencies \cite{8953830}.  

\paragraph{Graph convolution networks} Recently, existing models are further improved by the introduction of graph convolution networks (GCNs). Built on top of action segmentation models, the graph-based module models the temporal relations between initial segmentation results with temporal proximity. It refines the pre-computed action segments by performing segment boundary regression and segment classification \cite{graphbased2020}. The latest extension to this approach constructs multi-level dilated temporal graphs for temporal reasoning at different timescales \cite{wang2020temporal}. The limitations of the graph-based module exist in its dependence on the initial backbone output. For example, it suffers from low efficiency on large graphs if the initial segmentation is heavily fragmented. However, while abundant works have been done in unimodal action segmentation, we observe that almost none of the existing work attempts at multimodal approaches. Since textual data is one of the most common and accessible annotations of video data, we are interested in incorporating texts as a complimentary domain for better segmentation results.

\paragraph{Text alignment}
\iffalse
Covered paper:
- Multimodal Machine Learning: A Survey and Taxonomy
- Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning
- Unsupervised Learning from Narrated Instruction Videos
- What’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision
\fi
Identifying the relationship between two or more modalities is one of the core challenges in Multimodal settings \cite{MultiModalSurvey}. An example of the unsupervised approaches to text alignment is to first perform temporal clustering individually on the video input and the text input, then use the two clusters to provide complementary information to one another \cite{unsupervised-align}. For instance, differences in two video segments can provide a temporal cue to a breaking point within the narrative script. Contextual information is used to assist the alignment of textual scripts and video frames \cite{temporalAlignShi}. It is built by firstly collecting a mean pooling of each modality within $K$ units. The mean representation of two modalities is then combined through a transformer model and concatenated to the embedding of the individual. Our task concerns video and scripts in the cooking domain. In one of the similar experiments, the text script is parsed into action-object (i.e. verb-noun) classes, and the video frames are aligned to the text script by matching the tokens of action-objects to those present in the frames \cite{malmaud-etal-2015-whats}.

\paragraph{Text-image matching}
To build better representation for the graph nodes, we want to use the narrations to attend to regions in the frame that are closely associated with the action that is conducted. In this way, different relevant objects in different frames can be used to distinguish the segment boundaries. 

To use attention methods, we first need to provide a set of image features. To better extract object features in the images, Faster R-CNN \cite{ren2016faster} firstly generates Region Of Interests (ROIs) with high objectness, and it then uses intermediate convolution feature maps to classify the region and regress bounding boxes. Fully Convolutional One-Stage Object Detection (FCOS) \cite{tian2019fcos} belongs to a family of anchor-less methods. Instead of regressing bounding boxes using the anchors as references, it regresses four values, $l,t,r,b$ that represent the distance from a location in the image to the four sides of the bounding boxes. Moreover, it uses CNN feature maps from different levels to perform bounding box regression at various scales to capture objects with different sizes. It has been shown that anchor-less detectors perform better than anchor-based detectors on seen and unseen test sets, and FCOS can identify objects involved in the action \cite{yoon2020semisupervised}.  

Given a set of image features, encoding regions in the image, and a set of word features extracted from the sentence, Stacked Cross Attention \cite{lee2018stacked} determines the similarity between image-sentence pair by inferring how important a region is to the sentence, and it can also reversely infer how important a sentence is to the image. An additional position feature is concatenated for the object with the visual feature extracted by ResNet \cite{wang2019position}. The image is divided into blocks, and embedding vectors representing the positions of the blocks are combined with weights determined by overlap between the block and the visual feature. The addition is motivated by the fact that the positions of objects in the image are related to the semantics of the image. This intuition aligns with our task since we expect that the relative positions of objects are associated with the action during cooking.