We have four baseline models: both EDTCN \cite{8099596} and MS-TCN++ \cite{9186840} use temporal convolution networks to capture long-range dependencies. \newcite{8099623} proposed a hybrid usage of GRU-based RNN and HMM to refine action alignment. DTGRM \cite{wang2020temporal} uses multi-level dilated temporal graphs with an auxiliary self-supervised task to help correct wrong temporal relation in videos.

\paragraph{FC}
We implement a vanilla 2-layer fully connected neural network that performs frame-wise classification on the input video frames. The inputs are features of dimension 1024 extracted using pretrained I3D \cite{8099985}. We use this simple model to show the task complexity and the improvement of other baselines in both accuracy and fragmentation issues.

\paragraph{EDTCN and Dilated TCN}
EDTCN \cite{8099596} is the first work to present temporal convolution network (TCN), which aims to capture not only segmental features but also long-range patterns using a hierarchy of temporal convolution filters. It emphasizes the concept of receptive fields, which means a fixed-length input that the prediction output corresponds to. The encoder in the Encode-Decoder TCN (EDTCN) consists of layers of temporal convolutions, non-linear activation, and max pooling; the decoder has a similar structure, except that pooling is replaced with upsampling. Dilated TCN shares a similar structure with MS-TCN: a series of blocks, each with layers of dilated convolutions with exponentially growing dilation rates, and a residual connection between layer input and convolution signal.

\paragraph{MS-TCN++}
MS-TCN \cite{8953830} is a multi-stage architecture using TCN. The first layer of a single-stage TCN (SS-TCN) adjusts inputs dimension, followed by several dilated 1D temporal convolution layers with dilation factor doubled at each layer. All layers have ReLU activation with the residual connection. MS-TCN stacks four SS-TCNs so that each takes initial prediction probabilities from the previous stage and refines it. The overall architecture is trained with the cross entropy classification loss and a truncated mean squared error over the frame-wise log probabilities that penalizes over-segmentation. Extended from MS-TCN, MS-TCN++ \cite{9186840} decouples the prediction phase and the refinement phase and enables parameter sharing in the latter. Furthermore, it replaces the dilated layer with a dual dilated layer that combines two convolutions with different dilation factors in the first stage to addressing the limited receptive field. The SS-TCN in MS-TCN++ has 11 layers, each of which has 64 filters, kernel size 3 with a dropout rate of 0.5. The learning rate is set to be 0.0005 in training.


\paragraph{Weakly-supervised approach (RNN+HMM)}
\newcite{8099623} tackles the action alignment task by iteratively refining the coarsely proposed segmentation. Given a set of video frames and an ordered action sequence, the model assigns an action segment index to each frame. The initial proposal for boundaries is constructed by equally dividing the frames upon actions in the action sequence in the sequence's original order. The model uses a hybrid component of both a GRU-based RNN network and an HMM network to train and realign the proposal. To create more fine-grained proposals, the authors propose to further split each action into a sequence of subactions. The model attempts to create more fine-grained segmentation by modeling the probability distribution of the subactions within an action using the HMM network. To alleviate the computational need for recurrent neural networks used for processing videos, the authors propose to split the video frames into overlapping chunks. For instance, a single input would be a video frame at time $t$ and 20 frames before that. By splitting the input into smaller chunks, the model can take advantage of parallelism and utilize batch training.
% \paragraph{MSTCN+GTRM} Maybe?

\paragraph{DTGRM}

\newcite{wang2020temporal} proposed DTGRM which refines a predicted result given by the backbone model (e.g. I3D) iteratively. The model stacks $K$ dilated graph convolution layers to perform temporal reasoning across long timescales, where each layer updates the hidden representation of every input frame. To reduce over-segmentation error, an additional self-supervised task is introduced to simulate over-segmentation error by randomly exchanging part of input frames. Both the original and exchanged frame sequences are fed into the model as input, with the output being action class likelihood for two frame sequences as well as exchange likelihood for each frame. Since the model was trained on datasets with relatively shorter videos compared to EPIC-KITCHENS, we plan to trim the videos into overlapping clips of length 15 minutes with fixed fps for consistency.